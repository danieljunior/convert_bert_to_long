{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "convert_bert_to_long.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dcaled/convert_bert_to_long/blob/main/convert_bert_to_long.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aad_1s7ybD5o"
      },
      "source": [
        "# `BERT` --> `Longformer`: build a \"long\" version of pretrained models\n",
        "\n",
        "This notebook replicates the procedure descriped in the [Longformer paper](https://arxiv.org/abs/2004.05150) to train a Longformer model starting from the BERT checkpoint. The same procedure can be applied to build the \"long\" version of other pretrained models as well. It was inspired by the notebook provided by Allenai to convert RoBERTa to Longformer: [convert_model_to_long.ipynb](https://colab.research.google.com/github/allenai/longformer/blob/master/scripts/convert_model_to_long.ipynb).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BieXv0YUd7NF"
      },
      "source": [
        "### Libraries, and imports\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3yjIYKXw3rL",
        "outputId": "c1290c72-07d6-4060-b14c-51073d59701f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install transformers==3.0.2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==3.0.2 in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (1.18.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (0.0.43)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (20.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (3.0.12)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (0.1.94)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (0.8.1rc1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.2) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.2) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.2) (0.17.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (2020.6.20)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.0.2) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0NnMMl6wy7Q"
      },
      "source": [
        "import logging\n",
        "import os\n",
        "import math\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "from dataclasses import dataclass, field\n",
        "from transformers import AutoModel, AutoTokenizer, BertTokenizerFast, BertForMaskedLM, BertModel\n",
        "from transformers import TrainingArguments, HfArgumentParser\n",
        "from transformers.modeling_longformer import LongformerSelfAttention\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.basicConfig(level=logging.INFO)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgoNVJYUbD59"
      },
      "source": [
        "### BertLong\n",
        "\n",
        "`BertLongForMaskedLM` represents the \"long\" version of the `BERT` model. It replaces `BertSelfAttention` with `BertLongSelfAttention`, which is a thin wrapper around `LongformerSelfAttention`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9EBISkRxPjO"
      },
      "source": [
        "class BertLongSelfAttention(LongformerSelfAttention):\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        output_attentions=False,\n",
        "    ):\n",
        "        return super().forward(hidden_states, attention_mask=attention_mask, output_attentions=output_attentions)\n",
        "\n",
        "\n",
        "class BertLong(BertModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        for i, layer in enumerate(self.encoder.layer):\n",
        "            # replace the `modeling_bert.BertSelfAttention` object with `LongformerSelfAttention`\n",
        "            layer.attention.self = BertLongSelfAttention(config, layer_id=i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LRZa5s1bD6E"
      },
      "source": [
        "Starting from the `bert-base` checkpoint, the following function converts it into an instance of `BertLong`. It makes the following changes:\n",
        "\n",
        "- extend the position embeddings from `512` positions to `max_pos`. In Longformer, we set `max_pos=4096`\n",
        "\n",
        "- initialize the additional position embeddings by copying the embeddings of the first `512` positions. This initialization is crucial for the model performance (check table 6 in [the paper](https://arxiv.org/pdf/2004.05150.pdf) for performance without this initialization)\n",
        "\n",
        "- replaces `modeling_bert.BertSelfAttention` objects with `modeling_longformer.LongformerSelfAttention` with a attention window size `attention_window`\n",
        "\n",
        "The output of this function works for long documents even without pretraining. Check tables 6 and 11 in [the paper](https://arxiv.org/pdf/2004.05150.pdf) to get a sense of the expected performance of this model before pretraining."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-m4A_ttixPuf"
      },
      "source": [
        "def create_long_model(save_model_to, attention_window, max_pos):\n",
        "    model = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
        "    tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased', model_max_length=max_pos)\n",
        "    config = model.config\n",
        "\n",
        "    print(max_pos)\n",
        "    # extend position embeddings\n",
        "    tokenizer.model_max_length = max_pos\n",
        "    tokenizer.init_kwargs['model_max_length'] = max_pos\n",
        "    current_max_pos, embed_size = model.embeddings.position_embeddings.weight.shape\n",
        "    config.max_position_embeddings = max_pos\n",
        "    assert max_pos > current_max_pos\n",
        "    # allocate a larger position embedding matrix\n",
        "    new_pos_embed = model.embeddings.position_embeddings.weight.new_empty(max_pos, embed_size)\n",
        "    print(new_pos_embed.shape)\n",
        "    print(model.embeddings.position_embeddings)\n",
        "    # copy position embeddings over and over to initialize the new position embeddings\n",
        "    k = 0\n",
        "    step = current_max_pos\n",
        "    while k < max_pos - 1:\n",
        "        new_pos_embed[k:(k + step)] = model.embeddings.position_embeddings.weight\n",
        "        k += step\n",
        "    print(new_pos_embed.shape)\n",
        "    model.embeddings.position_ids = torch.from_numpy(tf.range(new_pos_embed.shape[0], dtype=tf.int32).numpy()[tf.newaxis, :])\n",
        "    model.embeddings.position_embeddings = torch.nn.Embedding.from_pretrained(new_pos_embed)\n",
        "    \n",
        "    # replace the `modeling_bert.BertSelfAttention` object with `LongformerSelfAttention`\n",
        "    config.attention_window = [attention_window] * config.num_hidden_layers\n",
        "    for i, layer in enumerate(model.encoder.layer):\n",
        "        longformer_self_attn = LongformerSelfAttention(config, layer_id=i)\n",
        "        longformer_self_attn.query = layer.attention.self.query\n",
        "        longformer_self_attn.key = layer.attention.self.key\n",
        "        longformer_self_attn.value = layer.attention.self.value\n",
        "\n",
        "        longformer_self_attn.query_global = layer.attention.self.query\n",
        "        longformer_self_attn.key_global = layer.attention.self.key\n",
        "        longformer_self_attn.value_global = layer.attention.self.value\n",
        "\n",
        "        layer.attention.self = longformer_self_attn\n",
        "    print(model.embeddings.position_ids.shape)\n",
        "    logger.info(f'saving model to {save_model_to}')\n",
        "    model.save_pretrained(save_model_to)\n",
        "    tokenizer.save_pretrained(save_model_to)\n",
        "    return model, tokenizer, new_pos_embed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqY_Hg5HbD6a"
      },
      "source": [
        "**Training hyperparameters**\n",
        "\n",
        "- Following BERT pretraining setting, we set number of tokens per batch to be `2^18` tokens. Changing this number might require changes in the lr, lr-scheudler, #steps and #warmup steps. Therefor, it is a good idea to keep this number constant.\n",
        "\n",
        "- Note that: `#tokens/batch = batch_size x #gpus x gradient_accumulation x seqlen`\n",
        "   \n",
        "- In [the paper](https://arxiv.org/pdf/2004.05150.pdf), we train for 65k steps, but 3k is probably enough (check table 6)\n",
        "\n",
        "- **Important note**: The lr-scheduler in [the paper](https://arxiv.org/pdf/2004.05150.pdf) is polynomial_decay with power 3 over 65k steps. To train for 3k steps, use a constant lr-scheduler (after warmup). Both lr-scheduler are not supported in HF trainer, and at least **constant lr-scheduler** will need to be added. \n",
        "\n",
        "- Pretraining will take 2 days on 1 x 32GB GPU with fp32. Consider using fp16 and using more gpus to train faster (if you increase `#gpus`, reduce `gradient_accumulation` to maintain `#tokens/batch` as mentioned earlier).\n",
        "\n",
        "- As a demonstration, this notebook is training on wikitext103 but wikitext103 is rather small that it takes 7 epochs to train for 3k steps Consider doing a single epoch on a larger dataset (800M tokens) instead.\n",
        "\n",
        "- Set #gpus using `CUDA_VISIBLE_DEVICES`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zl_hDDlryVo2"
      },
      "source": [
        "@dataclass\n",
        "class ModelArgs:\n",
        "    attention_window: int = field(default=512, metadata={\"help\": \"Size of attention window\"})\n",
        "    max_pos: int = field(default=4096, metadata={\"help\": \"Maximum position\"})\n",
        "\n",
        "parser = HfArgumentParser((TrainingArguments, ModelArgs,))\n",
        "\n",
        "\n",
        "training_args, model_args = parser.parse_args_into_dataclasses(look_for_args_file=False, args=[\n",
        "    '--output_dir', 'tmp',\n",
        "    '--warmup_steps', '500',\n",
        "    '--learning_rate', '0.00003',\n",
        "    '--weight_decay', '0.01',\n",
        "    '--adam_epsilon', '1e-6',\n",
        "    '--max_steps', '3000',\n",
        "    '--logging_steps', '500',\n",
        "    '--save_steps', '500',\n",
        "    '--max_grad_norm', '5.0',\n",
        "    '--per_gpu_eval_batch_size', '8',\n",
        "    '--per_gpu_train_batch_size', '2',  # 32GB gpu with fp32\n",
        "    '--gradient_accumulation_steps', '32',\n",
        "    '--evaluate_during_training',\n",
        "    '--do_train',\n",
        "    '--do_eval',\n",
        "])\n",
        "\n",
        "# Choose GPU\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBXU3r69bD6l"
      },
      "source": [
        "1) As descriped in `create_long_model`, convert a `bert-base` model into `bert-base-4096` which is an instance of `BertLong`, then save it to the disk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7tcsSfZ1-b9",
        "outputId": "38c1941c-7d00-41c2-ef5c-d4a3705e22b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model_path = f'{training_args.output_dir}/bert-base-{model_args.max_pos}'\n",
        "if not os.path.exists(model_path):\n",
        "    os.makedirs(model_path)\n",
        "\n",
        "logger.info(f'Converting bert-base into bert-base-{model_args.max_pos}')\n",
        "model, tokenizer, new_pos_embed = create_long_model(\n",
        "    save_model_to=model_path, attention_window=model_args.attention_window, max_pos=model_args.max_pos)\n",
        "#create_long_model(save_model_to, attention_window, max_pos)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:Converting bert-base into bert-base-4096\n",
            "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /root/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0\n",
            "INFO:transformers.configuration_utils:Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-cased-pytorch_model.bin from cache at /root/.cache/torch/transformers/3d1d2b2daef1e2b3ddc2180ddaae8b7a37d5f279babce0068361f71cd548f615.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059\n",
            "INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertModel.\n",
            "\n",
            "INFO:transformers.modeling_utils:All the weights of BertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\n",
            "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertModel for predictions without further training.\n",
            "INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /root/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "4096\n",
            "torch.Size([4096, 768])\n",
            "Embedding(512, 768)\n",
            "torch.Size([4096, 768])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:saving model to tmp/bert-base-4096\n",
            "INFO:transformers.configuration_utils:Configuration saved in tmp/bert-base-4096/config.json\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 4096])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:transformers.modeling_utils:Model weights saved in tmp/bert-base-4096/pytorch_model.bin\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiftMH3-zPUS"
      },
      "source": [
        "2) Load `bert-base-4096` from the disk. This model works for long sequences even without pretraining. If you don't want to pretrain, you can stop here and start finetuning your `bert-base-4096` on downstream tasks 🎉🎉🎉"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8vNeYdrzMd2",
        "outputId": "28a5170d-8147-4cc4-a6dd-a73bec482ee5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "logger.info(f'Loading the model from {model_path}')\n",
        "tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
        "model = BertLong.from_pretrained(model_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:Loading the model from tmp/bert-base-4096\n",
            "INFO:transformers.tokenization_utils_base:Model name 'tmp/bert-base-4096' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'tmp/bert-base-4096' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "INFO:transformers.tokenization_utils_base:Didn't find file tmp/bert-base-4096/added_tokens.json. We won't load it.\n",
            "INFO:transformers.tokenization_utils_base:Didn't find file tmp/bert-base-4096/tokenizer.json. We won't load it.\n",
            "INFO:transformers.tokenization_utils_base:loading file tmp/bert-base-4096/vocab.txt\n",
            "INFO:transformers.tokenization_utils_base:loading file None\n",
            "INFO:transformers.tokenization_utils_base:loading file tmp/bert-base-4096/special_tokens_map.json\n",
            "INFO:transformers.tokenization_utils_base:loading file tmp/bert-base-4096/tokenizer_config.json\n",
            "INFO:transformers.tokenization_utils_base:loading file None\n",
            "INFO:transformers.configuration_utils:loading configuration file tmp/bert-base-4096/config.json\n",
            "INFO:transformers.configuration_utils:Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"attention_window\": [\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512\n",
            "  ],\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "INFO:transformers.modeling_utils:loading weights file tmp/bert-base-4096/pytorch_model.bin\n",
            "INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertLong.\n",
            "\n",
            "INFO:transformers.modeling_utils:All the weights of BertLong were initialized from the model checkpoint at tmp/bert-base-4096.\n",
            "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertLong for predictions without further training.\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}