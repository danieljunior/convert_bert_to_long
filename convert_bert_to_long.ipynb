{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "convert_bert_to_long.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danieljunior/convert_bert_to_long/blob/main/convert_bert_to_long.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aad_1s7ybD5o"
      },
      "source": [
        "# `BERT` --> `Longformer`: build a \"long\" version of pretrained models\n",
        "\n",
        "This notebook replicates the procedure descriped in the [Longformer paper](https://arxiv.org/abs/2004.05150) to train a Longformer model starting from the BERT checkpoint. The same procedure can be applied to build the \"long\" version of other pretrained models as well. It was inspired by the notebook provided by Allenai to convert RoBERTa to Longformer: [convert_model_to_long.ipynb](https://colab.research.google.com/github/allenai/longformer/blob/master/scripts/convert_model_to_long.ipynb).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BieXv0YUd7NF"
      },
      "source": [
        "### Libraries, and imports\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3yjIYKXw3rL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df9f5e92-4e3f-4be5-8c5a-a541a3713cb0"
      },
      "source": [
        "!pip install transformers==3.0.2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==3.0.2 in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (0.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (3.0.12)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (0.1.94)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (1.19.4)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (0.0.43)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (20.8)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (0.8.1rc1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.2) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.2) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.2) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.0.2) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0NnMMl6wy7Q"
      },
      "source": [
        "import logging\n",
        "import os\n",
        "import math\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "from dataclasses import dataclass, field\n",
        "from transformers import AutoModel, AutoTokenizer, BertTokenizerFast, BertForMaskedLM, BertModel\n",
        "from transformers import TrainingArguments, HfArgumentParser\n",
        "from transformers.modeling_longformer import LongformerSelfAttention\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.basicConfig(level=logging.INFO)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgoNVJYUbD59"
      },
      "source": [
        "### BertLong\n",
        "\n",
        "`BertLongForMaskedLM` represents the \"long\" version of the `BERT` model. It replaces `BertSelfAttention` with `BertLongSelfAttention`, which is a thin wrapper around `LongformerSelfAttention`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9EBISkRxPjO"
      },
      "source": [
        "class BertLongSelfAttention(LongformerSelfAttention):\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        output_attentions=False,\n",
        "    ):\n",
        "        return super().forward(hidden_states, attention_mask=attention_mask, output_attentions=output_attentions)\n",
        "\n",
        "\n",
        "class BertLong(BertModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        for i, layer in enumerate(self.encoder.layer):\n",
        "            # replace the `modeling_bert.BertSelfAttention` object with `LongformerSelfAttention`\n",
        "            layer.attention.self = BertLongSelfAttention(config, layer_id=i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LRZa5s1bD6E"
      },
      "source": [
        "Starting from the `bert-base` checkpoint, the following function converts it into an instance of `BertLong`. It makes the following changes:\n",
        "\n",
        "- extend the position embeddings from `512` positions to `max_pos`. In Longformer, we set `max_pos=4096`\n",
        "\n",
        "- initialize the additional position embeddings by copying the embeddings of the first `512` positions. This initialization is crucial for the model performance (check table 6 in [the paper](https://arxiv.org/pdf/2004.05150.pdf) for performance without this initialization)\n",
        "\n",
        "- replaces `modeling_bert.BertSelfAttention` objects with `modeling_longformer.LongformerSelfAttention` with a attention window size `attention_window`\n",
        "\n",
        "The output of this function works for long documents even without pretraining. Check tables 6 and 11 in [the paper](https://arxiv.org/pdf/2004.05150.pdf) to get a sense of the expected performance of this model before pretraining."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-m4A_ttixPuf"
      },
      "source": [
        "def create_long_model(save_model_to, attention_window, max_pos):\n",
        "    model = BertModel.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
        "    tokenizer = BertTokenizerFast.from_pretrained('neuralmind/bert-base-portuguese-cased', model_max_length=max_pos)\n",
        "    config = model.config\n",
        "\n",
        "    print(max_pos)\n",
        "    # extend position embeddings\n",
        "    tokenizer.model_max_length = max_pos\n",
        "    tokenizer.init_kwargs['model_max_length'] = max_pos\n",
        "    current_max_pos, embed_size = model.embeddings.position_embeddings.weight.shape\n",
        "    config.max_position_embeddings = max_pos\n",
        "    assert max_pos > current_max_pos\n",
        "    # allocate a larger position embedding matrix\n",
        "    new_pos_embed = model.embeddings.position_embeddings.weight.new_empty(max_pos, embed_size)\n",
        "    print(new_pos_embed.shape)\n",
        "    print(model.embeddings.position_embeddings)\n",
        "    # copy position embeddings over and over to initialize the new position embeddings\n",
        "    k = 0\n",
        "    step = current_max_pos\n",
        "    while k < max_pos - 1:\n",
        "        new_pos_embed[k:(k + step)] = model.embeddings.position_embeddings.weight\n",
        "        k += step\n",
        "    print(new_pos_embed.shape)\n",
        "    model.embeddings.position_ids = torch.from_numpy(tf.range(new_pos_embed.shape[0], dtype=tf.int32).numpy()[tf.newaxis, :])\n",
        "    model.embeddings.position_embeddings = torch.nn.Embedding.from_pretrained(new_pos_embed)\n",
        "    \n",
        "    # replace the `modeling_bert.BertSelfAttention` object with `LongformerSelfAttention`\n",
        "    config.attention_window = [attention_window] * config.num_hidden_layers\n",
        "    for i, layer in enumerate(model.encoder.layer):\n",
        "        longformer_self_attn = LongformerSelfAttention(config, layer_id=i)\n",
        "        longformer_self_attn.query = layer.attention.self.query\n",
        "        longformer_self_attn.key = layer.attention.self.key\n",
        "        longformer_self_attn.value = layer.attention.self.value\n",
        "\n",
        "        longformer_self_attn.query_global = layer.attention.self.query\n",
        "        longformer_self_attn.key_global = layer.attention.self.key\n",
        "        longformer_self_attn.value_global = layer.attention.self.value\n",
        "\n",
        "        layer.attention.self = longformer_self_attn\n",
        "    print(model.embeddings.position_ids.shape)\n",
        "    logger.info(f'saving model to {save_model_to}')\n",
        "    model.save_pretrained(save_model_to)\n",
        "    tokenizer.save_pretrained(save_model_to)\n",
        "    return model, tokenizer, new_pos_embed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqY_Hg5HbD6a"
      },
      "source": [
        "**Training hyperparameters**\n",
        "\n",
        "- Following BERT pretraining setting, we set number of tokens per batch to be `2^18` tokens. Changing this number might require changes in the lr, lr-scheudler, #steps and #warmup steps. Therefor, it is a good idea to keep this number constant.\n",
        "\n",
        "- Note that: `#tokens/batch = batch_size x #gpus x gradient_accumulation x seqlen`\n",
        "   \n",
        "- In [the paper](https://arxiv.org/pdf/2004.05150.pdf), we train for 65k steps, but 3k is probably enough (check table 6)\n",
        "\n",
        "- **Important note**: The lr-scheduler in [the paper](https://arxiv.org/pdf/2004.05150.pdf) is polynomial_decay with power 3 over 65k steps. To train for 3k steps, use a constant lr-scheduler (after warmup). Both lr-scheduler are not supported in HF trainer, and at least **constant lr-scheduler** will need to be added. \n",
        "\n",
        "- Pretraining will take 2 days on 1 x 32GB GPU with fp32. Consider using fp16 and using more gpus to train faster (if you increase `#gpus`, reduce `gradient_accumulation` to maintain `#tokens/batch` as mentioned earlier).\n",
        "\n",
        "- As a demonstration, this notebook is training on wikitext103 but wikitext103 is rather small that it takes 7 epochs to train for 3k steps Consider doing a single epoch on a larger dataset (800M tokens) instead.\n",
        "\n",
        "- Set #gpus using `CUDA_VISIBLE_DEVICES`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zl_hDDlryVo2"
      },
      "source": [
        "@dataclass\n",
        "class ModelArgs:\n",
        "    attention_window: int = field(default=512, metadata={\"help\": \"Size of attention window\"})\n",
        "    max_pos: int = field(default=4096, metadata={\"help\": \"Maximum position\"})\n",
        "\n",
        "parser = HfArgumentParser((TrainingArguments, ModelArgs,))\n",
        "\n",
        "\n",
        "training_args, model_args = parser.parse_args_into_dataclasses(look_for_args_file=False, args=[\n",
        "    '--output_dir', 'tmp',\n",
        "    '--warmup_steps', '500',\n",
        "    '--learning_rate', '0.00003',\n",
        "    '--weight_decay', '0.01',\n",
        "    '--adam_epsilon', '1e-6',\n",
        "    '--max_steps', '3000',\n",
        "    '--logging_steps', '500',\n",
        "    '--save_steps', '500',\n",
        "    '--max_grad_norm', '5.0',\n",
        "    '--per_gpu_eval_batch_size', '8',\n",
        "    '--per_gpu_train_batch_size', '2',  # 32GB gpu with fp32\n",
        "    '--gradient_accumulation_steps', '32',\n",
        "    '--evaluate_during_training',\n",
        "    '--do_train',\n",
        "    '--do_eval',\n",
        "])\n",
        "\n",
        "# Choose GPU\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBXU3r69bD6l"
      },
      "source": [
        "1) As descriped in `create_long_model`, convert a `bert-base` model into `bert-base-4096` which is an instance of `BertLong`, then save it to the disk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7tcsSfZ1-b9"
      },
      "source": [
        "model_path = f'{training_args.output_dir}/bert-base-{model_args.max_pos}'\n",
        "if not os.path.exists(model_path):\n",
        "    os.makedirs(model_path)\n",
        "\n",
        "logger.info(f'Converting bert-base into bert-base-{model_args.max_pos}')\n",
        "model, tokenizer, new_pos_embed = create_long_model(\n",
        "    save_model_to=model_path, attention_window=model_args.attention_window, max_pos=model_args.max_pos)\n",
        "#create_long_model(save_model_to, attention_window, max_pos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiftMH3-zPUS"
      },
      "source": [
        "2) Load `bert-base-4096` from the disk. This model works for long sequences even without pretraining. If you don't want to pretrain, you can stop here and start finetuning your `bert-base-4096` on downstream tasks 🎉🎉🎉"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8vNeYdrzMd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de51eb11-eb61-495d-8435-bee753775459"
      },
      "source": [
        "logger.info(f'Loading the model from {model_path}')\n",
        "tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
        "model = BertLong.from_pretrained(model_path, output_hidden_states=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:Loading the model from tmp/bert-base-4096\n",
            "INFO:transformers.tokenization_utils_base:Model name 'tmp/bert-base-4096' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'tmp/bert-base-4096' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "INFO:transformers.tokenization_utils_base:Didn't find file tmp/bert-base-4096/added_tokens.json. We won't load it.\n",
            "INFO:transformers.tokenization_utils_base:Didn't find file tmp/bert-base-4096/tokenizer.json. We won't load it.\n",
            "INFO:transformers.tokenization_utils_base:loading file tmp/bert-base-4096/vocab.txt\n",
            "INFO:transformers.tokenization_utils_base:loading file None\n",
            "INFO:transformers.tokenization_utils_base:loading file tmp/bert-base-4096/special_tokens_map.json\n",
            "INFO:transformers.tokenization_utils_base:loading file tmp/bert-base-4096/tokenizer_config.json\n",
            "INFO:transformers.tokenization_utils_base:loading file None\n",
            "INFO:transformers.configuration_utils:loading configuration file tmp/bert-base-4096/config.json\n",
            "INFO:transformers.configuration_utils:Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"attention_window\": [\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512\n",
            "  ],\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_hidden_states\": true,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 29794\n",
            "}\n",
            "\n",
            "INFO:transformers.modeling_utils:loading weights file tmp/bert-base-4096/pytorch_model.bin\n",
            "INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertLong.\n",
            "\n",
            "INFO:transformers.modeling_utils:All the weights of BertLong were initialized from the model checkpoint at tmp/bert-base-4096.\n",
            "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertLong for predictions without further training.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xv1XCEOngLi9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d180d88f-2cc0-4a67-b882-fca2ee58a637"
      },
      "source": [
        "!pip install nltk\n",
        "import nltk    \n",
        "from nltk import tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "def convert_examples_to_features(example, seq_length, tokenizer):\n",
        "  \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
        "  tokens = ['[CLS]']\n",
        "  for i, w in enumerate(tokenize.word_tokenize(example, language='portuguese')):\n",
        "      # use bertTokenizer to split words\n",
        "      # 1996-08-22 => 1996 - 08 - 22\n",
        "      # sheepmeat => sheep ##me ##at\n",
        "      sub_words = tokenizer.tokenize(w)\n",
        "      if not sub_words:\n",
        "          sub_words = ['[UNK]']\n",
        "      # tokenize_count.append(len(sub_words))\n",
        "      tokens.extend(sub_words)\n",
        "\n",
        "  # truncate\n",
        "  if len(tokens) > seq_length - 1:\n",
        "      print('Example is too long, length is {}, truncated to {}!'.format(len(tokens), max_seq_length))\n",
        "      tokens = tokens[0:(seq_length - 1)]\n",
        "  tokens.append('[SEP]')\n",
        "\n",
        "  input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "  segment_ids = [0] * len(input_ids)\n",
        "  input_mask = [1] * len(input_ids)\n",
        "\n",
        "  while len(input_ids) < seq_length:\n",
        "    input_ids.append(0)\n",
        "    input_mask.append(0)\n",
        "    segment_ids.append(0)\n",
        "\n",
        "  return input_ids, segment_ids, input_mask\n",
        "\n",
        "\n",
        "def get_features(input_text, model, tokenizer, dim=768, max_lenght=4096):\n",
        "  input_ids, segment_ids, input_mask = convert_examples_to_features(\n",
        "      example=input_text, seq_length=max_lenght, tokenizer=tokenizer)\n",
        "  # import pdb; pdb.set_trace()\n",
        "  # unique_id_to_feature = {}\n",
        "  # for feature in features:\n",
        "  #   unique_id_to_feature[feature.unique_id] = feature\n",
        "  input_ids = torch.tensor([input_ids],dtype=torch.long)\n",
        "  segment_ids = torch.tensor([segment_ids],dtype=torch.long)\n",
        "  input_mask = torch.tensor([input_mask],dtype=torch.long)\n",
        "  outputs = model(input_ids, token_type_ids=segment_ids, \n",
        "                  attention_mask=input_mask)\n",
        "  # model_fn = model_fn_builder(\n",
        "  #     bert_config=bert_config,\n",
        "  #     init_checkpoint=INIT_CHECKPOINT,\n",
        "  #     layer_indexes=layer_indexes,\n",
        "  #     use_tpu=True,\n",
        "  #     use_one_hot_embeddings=True)\n",
        "\n",
        "  # # If TPU is not available, this will fall back to normal Estimator on CPU\n",
        "  # # or GPU.\n",
        "  # estimator = tf.contrib.tpu.TPUEstimator(\n",
        "  #     use_tpu=True,\n",
        "  #     model_fn=model_fn,\n",
        "  #     config=run_config,\n",
        "  #     predict_batch_size=BATCH_SIZE,\n",
        "  #     train_batch_size=BATCH_SIZE)\n",
        "\n",
        "  # input_fn = input_fn_builder(\n",
        "  #     features=features, seq_length=MAX_SEQ_LENGTH)\n",
        "\n",
        "  # # Get features\n",
        "  # for result in estimator.predict(input_fn, yield_single_examples=True):\n",
        "  #   unique_id = int(result[\"unique_id\"])\n",
        "  #   feature = unique_id_to_feature[unique_id]\n",
        "  #   output = collections.OrderedDict()\n",
        "  #   for (i, token) in enumerate(feature.tokens):\n",
        "  #     layers = []\n",
        "  #     for (j, layer_index) in enumerate(layer_indexes):\n",
        "  #       layer_output = result[\"layer_output_%d\" % j]\n",
        "  #       layer_output_flat = np.array([x for x in layer_output[i:(i + 1)].flat])\n",
        "  #       layers.append(layer_output_flat)\n",
        "  #     output[token] = sum(layers)[:dim]\n",
        "  \n",
        "  return outputs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGiyKB7_jTMD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6dd6e72-8b51-4f2d-f300-74fce07010cd"
      },
      "source": [
        "embeddings = get_features('Olá, essa é uma sentença de teste', model, tokenizer)\n",
        "print(embeddings)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(tensor([[[ 0.2329, -0.1494,  0.3605,  ..., -0.1701, -0.2105, -0.5222],\n",
            "         [-0.1091, -0.5142,  0.4510,  ...,  0.1104,  0.0641, -0.2985],\n",
            "         [ 0.1689, -0.6535,  0.4545,  ...,  0.1635, -0.0096, -0.5424],\n",
            "         ...,\n",
            "         [-0.1672,  0.0212,  0.5901,  ..., -0.0417, -0.0309, -0.3464],\n",
            "         [-0.1043, -0.0432,  0.6870,  ...,  0.0186, -0.0413, -0.5592],\n",
            "         [ 0.0080, -0.0725,  0.3609,  ..., -0.3591,  0.0622, -0.3122]]],\n",
            "       grad_fn=<NativeLayerNormBackward>), tensor([[ 2.9375e-02, -6.5288e-02,  5.5953e-02,  1.0286e-01, -2.8127e-02,\n",
            "          1.3449e-01,  9.8631e-01, -1.3504e-02,  2.4237e-02, -3.5371e-02,\n",
            "          4.5435e-01,  1.0632e-01,  8.8997e-02, -1.6181e-01,  3.1021e-02,\n",
            "         -9.1601e-02, -6.5733e-02, -2.9517e-03, -4.2562e-01,  9.9206e-01,\n",
            "         -2.6046e-01,  1.8278e-02,  1.0318e-01, -5.5111e-02, -5.8931e-02,\n",
            "          3.3217e-02,  9.4834e-02, -1.0454e-01, -1.1867e-01, -8.0529e-02,\n",
            "          1.2652e-02, -9.8999e-01,  3.1234e-01, -1.4664e-01,  8.2998e-02,\n",
            "          7.7581e-02, -2.5918e-02,  2.4867e-02, -4.6780e-01,  2.6385e-02,\n",
            "          3.0093e-03, -1.7650e-01, -2.9267e-01,  7.5159e-02,  5.6001e-03,\n",
            "          1.8957e-01,  1.8461e-01, -7.8689e-02,  1.5309e-01, -1.1604e-01,\n",
            "         -1.1706e-01,  8.8035e-02,  2.0713e-02, -1.2717e-01, -9.8738e-04,\n",
            "         -5.2586e-02, -1.2820e-01, -8.6622e-02,  2.7556e-02, -7.2265e-02,\n",
            "          7.5827e-02, -3.7063e-02, -1.5958e-02, -1.2875e-03, -9.7632e-02,\n",
            "         -4.1260e-02, -5.9168e-02,  7.1242e-02, -2.4480e-01, -3.3686e-02,\n",
            "          9.1216e-02, -7.4081e-01,  1.4496e-01, -1.3609e-02,  5.7002e-02,\n",
            "         -3.1753e-01,  9.2707e-02, -1.1421e-01,  1.3076e-01, -1.0743e-02,\n",
            "          7.6075e-02,  1.8636e-02,  2.3166e-02, -1.7566e-01,  1.1485e-01,\n",
            "          2.0896e-02, -2.0264e-02, -4.0822e-03,  4.7504e-02, -1.9835e-01,\n",
            "          9.8724e-02,  3.4008e-01, -1.8759e-01,  1.3359e-02, -8.0703e-02,\n",
            "          8.7034e-03, -6.3284e-03,  1.9728e-02,  6.0739e-03,  1.6450e-02,\n",
            "         -9.4191e-03, -3.0820e-02, -2.1980e-02,  1.0631e-01,  3.4841e-02,\n",
            "          7.5644e-01,  1.1548e-01, -1.7634e-01,  5.3984e-02,  6.5266e-02,\n",
            "          5.0006e-02, -2.1422e-02,  8.7699e-02, -2.4950e-02, -3.9060e-02,\n",
            "         -1.4415e-01,  1.6869e-02,  3.0869e-02,  1.7201e-02,  2.7638e-02,\n",
            "          3.7206e-02, -4.7109e-02, -5.4407e-02,  1.4574e-01, -9.9464e-02,\n",
            "         -9.8042e-02,  5.5275e-02,  5.5334e-03, -2.8690e-03,  2.2487e-02,\n",
            "          8.7173e-01,  7.3292e-02,  6.6165e-02,  3.8066e-02,  9.6140e-02,\n",
            "         -8.5336e-02, -6.6465e-02, -3.0240e-02,  9.0195e-01, -4.2666e-02,\n",
            "         -5.3854e-02,  5.4220e-02,  6.0360e-03,  1.5621e-01,  2.6367e-02,\n",
            "         -7.3743e-02,  2.0097e-02,  3.1857e-02,  9.8575e-01,  1.9357e-01,\n",
            "         -7.2370e-02, -2.9785e-03,  2.1172e-01,  1.1398e-01,  1.1317e-01,\n",
            "         -2.9026e-02,  4.2821e-02, -2.5809e-03,  7.0106e-02,  1.0290e-01,\n",
            "         -6.5595e-02, -9.8292e-02,  7.0992e-02,  1.0553e-02, -2.0841e-02,\n",
            "         -3.5627e-01, -2.2061e-01, -1.8529e-01,  1.0801e-01,  7.5089e-02,\n",
            "         -1.3218e-01,  8.8148e-02,  5.0005e-02, -7.2782e-02, -2.0851e-02,\n",
            "          4.0778e-02, -2.2427e-01, -7.6294e-02, -1.1203e-01, -4.2362e-03,\n",
            "          8.0751e-02,  8.1471e-02,  5.1475e-02,  4.2302e-02,  8.1210e-02,\n",
            "         -1.9424e-01,  1.8667e-01, -9.5591e-02, -4.9777e-02, -4.8567e-02,\n",
            "          7.7871e-02,  9.6173e-02,  1.1980e-01,  2.9426e-02,  1.0996e-01,\n",
            "          4.2233e-02, -2.9250e-02, -4.8770e-02, -1.9160e-02, -1.2873e-01,\n",
            "         -3.1387e-02, -9.2574e-02,  1.3788e-02, -6.9889e-02, -4.9675e-01,\n",
            "         -2.9265e-01, -4.1089e-02, -3.2790e-02,  5.1011e-02,  6.3827e-02,\n",
            "          9.9886e-01,  1.6270e-02, -8.7313e-02, -9.5153e-01,  1.1809e-01,\n",
            "          1.5390e-01,  1.4662e-01,  1.6205e-02, -4.8332e-02, -1.4709e-01,\n",
            "         -3.4133e-02,  3.3302e-02, -1.8660e-01,  5.5178e-02,  1.7251e-01,\n",
            "          7.6243e-02,  5.8438e-02, -2.4104e-02, -7.0919e-03,  8.0401e-02,\n",
            "          3.3459e-02, -3.3409e-02,  2.9346e-02,  5.0667e-02, -4.2738e-02,\n",
            "         -8.1571e-01,  1.1419e-01, -6.6884e-02, -4.7317e-01,  1.7014e-01,\n",
            "         -9.2959e-02,  9.9784e-01,  4.7454e-03,  9.6683e-01,  6.4582e-02,\n",
            "         -7.9748e-02,  4.0214e-02, -1.6618e-02,  7.4513e-02, -2.0054e-01,\n",
            "          9.9861e-01,  1.1983e-01,  1.6755e-01, -3.7324e-02, -1.0141e-01,\n",
            "         -9.6477e-01,  3.2333e-02,  6.2313e-02,  2.9078e-02, -6.4586e-02,\n",
            "         -2.3056e-02,  1.7774e-01,  5.3179e-02,  1.5785e-01,  7.5732e-01,\n",
            "          1.0726e-01,  8.3440e-02, -2.4523e-03, -1.3106e-01, -9.7137e-03,\n",
            "         -1.2806e-01,  2.5662e-02, -1.6633e-02,  1.2907e-01,  5.8027e-03,\n",
            "         -4.5882e-02,  2.0864e-02, -3.1094e-01, -1.1119e-01,  4.3135e-02,\n",
            "         -1.6517e-01,  5.5165e-02, -8.9335e-02, -9.1076e-02,  8.2626e-02,\n",
            "         -1.1357e-01, -1.4092e-02, -1.5171e-01,  2.6835e-01,  1.5682e-01,\n",
            "          6.4823e-02, -1.2318e-01, -3.7204e-02,  7.3653e-02,  9.5001e-03,\n",
            "         -4.6717e-01, -6.4203e-02,  3.2903e-02,  4.9201e-02,  3.5973e-02,\n",
            "          1.1218e-01,  1.2794e-01,  1.3605e-02, -9.9944e-01,  9.7020e-02,\n",
            "         -3.9481e-03,  7.6359e-02, -9.9674e-01,  2.2341e-02,  1.1294e-01,\n",
            "          7.5256e-02,  7.5364e-02,  1.6438e-01, -5.6445e-02, -9.4211e-02,\n",
            "         -1.5630e-01,  9.4962e-01,  4.6693e-02,  6.4921e-01, -1.9712e-02,\n",
            "          2.1049e-03,  2.0589e-01, -3.1876e-02,  9.8393e-01, -1.1106e-01,\n",
            "         -1.7072e-02, -6.0502e-02, -9.1002e-01,  2.5228e-01,  5.1734e-02,\n",
            "         -9.4436e-01,  8.9772e-01,  1.9983e-01, -9.8948e-01, -1.2819e-02,\n",
            "         -5.6554e-02, -7.0492e-01, -1.3920e-01, -1.5568e-01, -9.0052e-02,\n",
            "         -1.2040e-02,  3.1966e-02,  6.6720e-01,  1.7422e-01,  3.3864e-02,\n",
            "          1.2143e-01,  7.1938e-02, -4.4711e-02, -1.0171e-01,  8.3535e-02,\n",
            "          2.6678e-01, -5.6714e-02, -3.1053e-02, -7.3607e-02,  3.1291e-02,\n",
            "          1.8784e-01, -4.9855e-02, -9.5666e-02, -5.1134e-02, -9.3177e-02,\n",
            "          1.8439e-01,  7.8984e-03,  6.2261e-03,  1.2080e-01,  5.7512e-02,\n",
            "          1.0581e-01, -1.3046e-01, -2.0084e-02,  2.7008e-02,  2.7530e-02,\n",
            "          1.2718e-01, -9.9006e-01, -8.7040e-02,  4.2630e-02,  1.3767e-01,\n",
            "          9.1776e-02,  5.9496e-01, -8.9224e-02,  1.2450e-01, -1.4532e-01,\n",
            "          7.2708e-02,  9.2992e-02,  2.0754e-04, -1.6758e-01, -7.9006e-02,\n",
            "         -1.6258e-02, -8.4123e-01,  7.0832e-01, -3.9635e-02, -9.9842e-02,\n",
            "          1.7085e-02,  1.0019e-01,  9.6068e-01,  1.9841e-01, -1.0684e-01,\n",
            "          2.3406e-02,  1.2530e-01,  8.2003e-02,  7.3522e-04,  8.7287e-02,\n",
            "         -1.4321e-01, -4.7181e-02,  8.9307e-02,  9.8662e-02,  6.3047e-02,\n",
            "          2.7236e-02,  4.3983e-01,  1.4688e-01,  4.5765e-02,  3.0837e-02,\n",
            "         -2.8533e-02, -8.8364e-02, -2.9893e-02, -6.1578e-01,  9.0707e-02,\n",
            "         -3.3179e-02,  1.4782e-01, -8.0189e-02, -1.4740e-02,  1.8248e-02,\n",
            "          7.1811e-01,  9.8394e-01,  4.1622e-02, -2.2632e-01, -6.2622e-02,\n",
            "          9.9321e-02, -2.4599e-02, -6.7060e-02, -3.5991e-02, -6.1101e-02,\n",
            "          8.1473e-02,  3.5753e-02,  6.5273e-02, -4.0160e-02,  6.6607e-02,\n",
            "         -2.1215e-03,  2.0367e-01, -5.0666e-02, -2.9074e-03, -6.2523e-02,\n",
            "         -2.9372e-02,  1.6548e-01, -9.9881e-02, -6.7947e-02,  2.5688e-02,\n",
            "         -1.2052e-01,  9.6366e-03,  4.5649e-02, -6.5189e-02, -2.7876e-02,\n",
            "          7.8894e-03, -4.1337e-02, -7.4522e-02, -5.6819e-02,  4.8360e-02,\n",
            "          1.8454e-01,  9.8830e-01,  6.6756e-01,  4.9110e-02,  6.9255e-02,\n",
            "          4.5637e-02,  2.5706e-02,  7.8009e-02,  1.0555e-01, -8.6587e-02,\n",
            "         -1.0402e-01, -9.7215e-02,  1.1268e-01,  4.6653e-02,  1.1203e-01,\n",
            "         -9.0098e-01,  7.0208e-02, -9.9967e-01, -7.9630e-02, -5.0540e-02,\n",
            "          3.7378e-02,  6.9112e-01, -9.5876e-02, -2.7566e-02, -1.1731e-01,\n",
            "          1.5276e-01, -2.7298e-02, -1.2656e-01, -1.1530e-01, -9.0019e-02,\n",
            "          1.5733e-01,  5.9250e-02,  1.9738e-01,  4.2240e-02,  8.8100e-02,\n",
            "         -5.3004e-02,  1.0963e-01,  1.1615e-01, -2.3056e-01,  3.6377e-01,\n",
            "         -1.2750e-02, -1.6910e-03, -7.5968e-02,  2.8558e-03,  1.2341e-02,\n",
            "          7.4537e-01, -1.0748e-01, -2.1909e-02,  3.8407e-02, -4.2448e-02,\n",
            "          3.0150e-01, -9.4933e-02, -1.1918e-02, -8.0267e-01, -6.6938e-02,\n",
            "         -1.0765e-01,  1.0971e-01,  1.4799e-01,  7.6284e-02,  9.4500e-01,\n",
            "         -8.9821e-02,  1.0727e-01, -3.5466e-03, -2.2735e-01, -7.7943e-02,\n",
            "         -2.6296e-02,  9.2158e-02,  2.1953e-02,  1.6744e-02, -7.8514e-02,\n",
            "          9.9276e-01, -5.6323e-02, -1.4284e-02, -2.7989e-02,  2.9735e-03,\n",
            "         -2.1768e-02, -9.5136e-02,  5.2774e-02,  9.7524e-01,  9.3332e-02,\n",
            "          2.0459e-01, -4.8177e-02,  9.9710e-01, -6.7938e-02, -6.5279e-02,\n",
            "         -1.1373e-02, -1.6744e-01, -5.2289e-02,  4.5843e-02, -1.0190e-01,\n",
            "          5.6046e-01,  2.3027e-02,  1.4927e-01,  9.3854e-02,  8.1799e-02,\n",
            "          1.6516e-01,  7.0251e-02, -2.8545e-02,  1.2107e-01, -9.8631e-02,\n",
            "          7.4255e-02,  3.8914e-01, -3.0466e-01, -1.3337e-01, -3.4881e-02,\n",
            "          4.2022e-02, -8.9060e-02,  1.0114e-01, -7.9776e-02,  9.9061e-01,\n",
            "          5.6773e-02, -2.0805e-01,  1.1271e-01, -9.1296e-02, -1.1363e-02,\n",
            "          4.5186e-02,  1.3393e-01,  4.4072e-02,  3.9845e-02,  6.1836e-02,\n",
            "          1.1928e-01, -6.8585e-02,  1.5020e-01, -1.7117e-01, -1.6059e-01,\n",
            "          1.1713e-01, -1.2632e-01, -9.5469e-03,  1.0095e-01,  8.5582e-03,\n",
            "         -9.4333e-02, -1.1160e-01, -1.0740e-01,  2.1753e-02,  5.6748e-02,\n",
            "         -1.1812e-01,  2.7758e-02, -1.4486e-01, -1.6362e-02,  1.6987e-01,\n",
            "          2.4543e-03, -3.0506e-02, -1.7420e-01, -7.0510e-02, -1.3687e-02,\n",
            "         -2.2007e-02,  6.6017e-02, -3.6053e-02, -3.0738e-02,  1.4154e-01,\n",
            "          6.1959e-02, -8.4218e-01,  1.6234e-01, -1.0012e-01,  1.5737e-01,\n",
            "         -4.0165e-02, -6.9998e-03,  9.3007e-02,  2.1900e-02, -3.1116e-01,\n",
            "         -1.4224e-01, -1.4925e-01,  6.1519e-02,  1.1061e-01,  2.1693e-01,\n",
            "         -8.8320e-01, -2.2004e-01,  3.9809e-02, -7.4033e-02,  6.4882e-01,\n",
            "          1.5699e-02,  1.1992e-01, -3.5359e-02,  3.4486e-02, -4.6691e-02,\n",
            "         -1.2899e-01,  4.7455e-02, -9.8739e-02,  7.3669e-02,  1.9449e-01,\n",
            "          9.4967e-02,  2.6972e-02,  1.6199e-01,  2.1139e-02,  2.0093e-01,\n",
            "          3.4620e-01, -5.4846e-02, -8.4876e-02, -4.4982e-03,  2.2968e-01,\n",
            "          4.5332e-02,  5.3867e-02,  9.9986e-02,  1.8325e-01, -1.9017e-03,\n",
            "          3.3768e-02,  5.4608e-02, -7.4891e-02, -2.1262e-02,  6.9107e-02,\n",
            "          1.8124e-01,  1.3268e-01, -2.0797e-01, -1.2906e-01, -8.1972e-01,\n",
            "         -1.3330e-01, -1.5630e-02, -6.5769e-02,  1.1079e-01, -1.1003e-02,\n",
            "          2.1757e-02, -3.3847e-02,  1.8651e-02, -9.1799e-02, -2.4840e-01,\n",
            "         -1.4976e-02,  8.8072e-02, -1.3129e-01, -1.8884e-02,  9.4809e-02,\n",
            "         -8.1332e-03,  1.5949e-01, -1.1416e-01, -4.2586e-02, -1.5195e-01,\n",
            "          5.6181e-01,  5.2763e-02, -1.3785e-01,  5.5634e-02,  3.0037e-02,\n",
            "          1.3268e-01, -2.1138e-01,  2.4088e-02, -8.4723e-02, -2.7010e-02,\n",
            "         -2.0621e-01, -2.1540e-02,  7.3121e-02, -1.1835e-02,  6.6653e-02,\n",
            "          1.0646e-01,  1.7470e-02,  1.2626e-01, -4.2034e-02, -2.6610e-02,\n",
            "          1.0318e-01,  3.4716e-02,  5.2010e-02, -8.5706e-03,  3.0264e-02,\n",
            "          2.0351e-01, -1.9640e-01, -7.0003e-02,  4.1406e-01,  2.3794e-02,\n",
            "         -1.3489e-01,  1.8678e-02, -2.6129e-02, -6.3665e-03, -7.3989e-02,\n",
            "         -1.9470e-01,  1.0439e-02, -2.3061e-02, -3.3996e-02, -5.7633e-03,\n",
            "         -4.2916e-02, -8.6574e-01, -3.2988e-02, -8.1988e-02, -6.4674e-02,\n",
            "         -1.6966e-01, -9.6059e-03,  9.5535e-02,  3.1521e-02, -6.1745e-02,\n",
            "          2.0660e-01,  9.1002e-03,  5.6067e-03,  1.6719e-01,  4.4314e-02,\n",
            "          2.4142e-02, -7.1057e-02,  3.0396e-02,  1.0650e-01,  1.3738e-01,\n",
            "          1.7672e-01, -8.6452e-02,  1.6391e-02,  1.1488e-01, -6.3544e-02,\n",
            "         -1.0559e-01,  3.4914e-02, -1.1049e-02,  9.4778e-01,  1.3625e-01,\n",
            "         -6.1995e-03,  9.8245e-02, -5.0157e-02, -2.3699e-01,  1.0856e-01,\n",
            "          5.9892e-02,  4.0017e-01,  5.4484e-02, -7.1621e-02, -5.2608e-02,\n",
            "          1.9127e-02,  8.6911e-01, -4.2864e-02,  9.1614e-02, -5.8544e-03,\n",
            "          9.0784e-02,  7.7500e-02, -1.6965e-01]], grad_fn=<TanhBackward>), (tensor([[[-0.2793,  0.0076, -0.0980,  ...,  0.1093, -0.1878, -0.0169],\n",
            "         [ 0.3820,  0.0109,  0.4327,  ...,  0.9207,  0.2561, -0.3224],\n",
            "         [-0.1036, -0.4065,  0.1936,  ..., -1.0570,  0.5271,  0.1770],\n",
            "         ...,\n",
            "         [ 0.4723,  0.5205,  1.1534,  ..., -0.7937, -0.4280,  1.3064],\n",
            "         [ 0.5995,  0.3311,  1.1280,  ..., -0.9991, -0.9351,  0.7917],\n",
            "         [ 0.0633,  0.4051,  0.8920,  ..., -0.6749, -1.0033,  0.7356]]],\n",
            "       grad_fn=<NativeLayerNormBackward>), tensor([[[ 8.7379e-03, -7.6372e-04,  4.4114e-02,  ...,  8.1612e-04,\n",
            "          -4.3437e-02,  1.3730e-01],\n",
            "         [ 4.0620e-01, -6.0680e-01,  2.0177e-01,  ...,  1.0263e+00,\n",
            "           1.0053e-01, -5.9077e-01],\n",
            "         [-8.9555e-01, -4.0469e-01,  3.7343e-01,  ..., -7.3042e-01,\n",
            "           3.1109e-01, -1.2209e-01],\n",
            "         ...,\n",
            "         [ 5.1883e-01,  3.5404e-01,  1.1239e+00,  ..., -7.0209e-01,\n",
            "          -4.6263e-01,  1.4065e+00],\n",
            "         [ 5.5734e-01,  8.0222e-02,  1.1720e+00,  ..., -9.4559e-01,\n",
            "          -1.0568e+00,  9.4421e-01],\n",
            "         [ 1.3168e-01,  4.4141e-01,  8.9376e-01,  ..., -6.1990e-01,\n",
            "          -9.1480e-01,  6.1482e-01]]], grad_fn=<NativeLayerNormBackward>), tensor([[[-0.0079, -0.0290,  0.0699,  ..., -0.0063, -0.0159,  0.0486],\n",
            "         [ 0.5394, -0.5443,  0.2548,  ...,  0.5744, -0.3690, -0.5692],\n",
            "         [-0.6750, -0.5090, -0.2666,  ..., -0.1936,  0.0347,  0.4141],\n",
            "         ...,\n",
            "         [ 0.4671,  0.1635,  0.7289,  ..., -0.7892, -0.0735,  1.1548],\n",
            "         [ 0.3765,  0.0642,  1.0040,  ..., -0.9330, -0.7284,  0.8145],\n",
            "         [ 0.0337, -0.0501,  0.9579,  ..., -0.5915, -0.6113,  0.5383]]],\n",
            "       grad_fn=<NativeLayerNormBackward>), tensor([[[-0.0314,  0.0482,  0.0405,  ..., -0.0530, -0.0048,  0.1176],\n",
            "         [-0.0709, -0.1869,  0.1754,  ...,  0.3895, -0.5104,  0.1559],\n",
            "         [-0.3969, -0.8274,  0.4059,  ..., -0.0833,  0.6739,  0.8416],\n",
            "         ...,\n",
            "         [ 0.2395,  0.1593,  0.7855,  ..., -0.4893, -0.2081,  0.8438],\n",
            "         [ 0.1666, -0.0609,  1.3053,  ..., -0.7156, -0.9741,  0.4936],\n",
            "         [-0.0880,  0.0806,  1.0242,  ..., -0.5973, -0.6602,  0.4159]]],\n",
            "       grad_fn=<NativeLayerNormBackward>), tensor([[[-0.0348, -0.0053,  0.0939,  ..., -0.0291,  0.0105,  0.0461],\n",
            "         [-0.1727, -0.6578,  0.1738,  ...,  0.2491, -0.2309, -0.0506],\n",
            "         [-0.3888, -0.5838,  0.6216,  ...,  0.5149,  0.3163,  0.2529],\n",
            "         ...,\n",
            "         [-0.1409,  0.4151,  0.6633,  ..., -0.3519, -0.0979,  0.3655],\n",
            "         [-0.1165,  0.4337,  1.0958,  ..., -0.6157, -0.6033,  0.1450],\n",
            "         [-0.0648,  0.2198,  0.8970,  ..., -0.6498, -0.4853,  0.2300]]],\n",
            "       grad_fn=<NativeLayerNormBackward>), tensor([[[ 0.0117,  0.0241,  0.0574,  ..., -0.0760,  0.0165,  0.0238],\n",
            "         [-0.2606, -0.5363,  0.3153,  ...,  0.1975, -0.3828,  0.3226],\n",
            "         [-0.4449, -0.1789,  0.3339,  ..., -0.0571, -0.1250,  0.4486],\n",
            "         ...,\n",
            "         [-0.3650, -0.0750,  0.9149,  ..., -0.3145, -0.5155, -0.0176],\n",
            "         [-0.2894, -0.1577,  1.2912,  ..., -0.7610, -1.0378, -0.3053],\n",
            "         [-0.0508, -0.1725,  1.1209,  ..., -0.7970, -0.9486, -0.1133]]],\n",
            "       grad_fn=<NativeLayerNormBackward>), tensor([[[-0.0062, -0.0301,  0.0262,  ..., -0.0616,  0.0068,  0.0097],\n",
            "         [-0.3992, -0.6067,  0.8474,  ...,  0.4473, -0.0462,  0.4099],\n",
            "         [-0.0492, -0.3994,  0.2596,  ...,  0.1955,  0.4001,  0.8628],\n",
            "         ...,\n",
            "         [-0.2736, -0.0185,  0.8803,  ..., -0.2358, -0.6065, -0.1820],\n",
            "         [-0.0592, -0.0158,  1.2215,  ..., -0.2494, -1.0107, -0.3864],\n",
            "         [ 0.4347,  0.1106,  1.4125,  ..., -0.2134, -1.0273, -0.5669]]],\n",
            "       grad_fn=<NativeLayerNormBackward>), tensor([[[-5.1854e-04, -1.6510e-02,  1.0155e-02,  ..., -8.9489e-02,\n",
            "          -2.7654e-02,  9.7780e-04],\n",
            "         [-3.2656e-01, -1.7783e-01,  8.2787e-01,  ...,  7.6282e-01,\n",
            "          -1.1337e-02,  4.1611e-01],\n",
            "         [-1.7207e-01, -1.5561e-02,  6.4509e-01,  ...,  9.2704e-01,\n",
            "           4.4443e-01,  1.0565e+00],\n",
            "         ...,\n",
            "         [-1.7266e-02,  4.6743e-01,  4.0729e-01,  ..., -2.1862e-01,\n",
            "          -5.3948e-01,  1.6994e-01],\n",
            "         [ 3.4997e-01,  5.6068e-01,  8.3721e-01,  ..., -2.5951e-01,\n",
            "          -8.0348e-01, -3.7467e-01],\n",
            "         [ 5.2050e-01,  4.4897e-01,  8.7684e-01,  ..., -1.8251e-01,\n",
            "          -3.7605e-01, -6.3909e-01]]], grad_fn=<NativeLayerNormBackward>), tensor([[[-0.0303,  0.0549, -0.0255,  ..., -0.1178, -0.1172,  0.1454],\n",
            "         [-0.1291, -0.2498,  0.5507,  ...,  0.9453, -0.1130,  0.4127],\n",
            "         [ 0.2873, -0.1559,  0.7890,  ...,  1.1572,  0.3385,  1.0062],\n",
            "         ...,\n",
            "         [-0.1287,  0.4199,  0.4780,  ..., -0.1478, -0.6130, -0.2339],\n",
            "         [ 0.0730,  0.3344,  1.0250,  ..., -0.1634, -0.7203, -0.7833],\n",
            "         [ 0.2650,  0.1480,  0.7758,  ..., -0.1171,  0.0328, -0.5482]]],\n",
            "       grad_fn=<NativeLayerNormBackward>), tensor([[[-0.0041, -0.0337,  0.0073,  ..., -0.1714, -0.0812,  0.1047],\n",
            "         [-0.0599, -0.4751,  0.4414,  ...,  0.7381,  0.1684,  0.4585],\n",
            "         [ 0.3482, -0.0030,  0.6041,  ...,  0.8889,  0.6791,  0.7120],\n",
            "         ...,\n",
            "         [ 0.1717,  0.8314,  0.2178,  ..., -0.0247, -0.2966, -0.2364],\n",
            "         [ 0.3989,  0.6846,  0.6126,  ...,  0.1472, -0.3785, -0.8929],\n",
            "         [ 0.0547,  0.0771,  0.1286,  ..., -0.1614, -0.0840, -0.0090]]],\n",
            "       grad_fn=<NativeLayerNormBackward>), tensor([[[ 0.0201, -0.0096,  0.0526,  ..., -0.1245, -0.0358,  0.0654],\n",
            "         [-0.1929, -0.6996, -0.1756,  ...,  0.7477,  0.3069,  0.3615],\n",
            "         [ 0.2759, -0.3211, -0.0614,  ...,  0.7347,  0.7089,  0.3268],\n",
            "         ...,\n",
            "         [ 0.0644,  0.5199,  0.4711,  ...,  0.1870,  0.3034, -0.3813],\n",
            "         [ 0.3163,  0.4187,  0.7145,  ...,  0.3821,  0.2955, -0.8729],\n",
            "         [ 0.0259,  0.0011,  0.0501,  ..., -0.1344, -0.0389,  0.0510]]],\n",
            "       grad_fn=<NativeLayerNormBackward>), tensor([[[ 0.0310, -0.0147,  0.0599,  ..., -0.1117, -0.0202,  0.0505],\n",
            "         [-0.0021, -0.6958, -0.3024,  ...,  0.4615,  0.0742,  0.3878],\n",
            "         [ 0.4161, -0.5010, -0.1547,  ...,  0.6630,  0.4190,  0.2243],\n",
            "         ...,\n",
            "         [-0.2056,  0.4162,  0.2333,  ...,  0.2339,  0.0609, -0.1661],\n",
            "         [ 0.0067,  0.3644,  0.4268,  ...,  0.4073,  0.0456, -0.6153],\n",
            "         [ 0.0204, -0.0113,  0.0648,  ..., -0.1153, -0.0268,  0.0540]]],\n",
            "       grad_fn=<NativeLayerNormBackward>), tensor([[[ 0.2329, -0.1494,  0.3605,  ..., -0.1701, -0.2105, -0.5222],\n",
            "         [-0.1091, -0.5142,  0.4510,  ...,  0.1104,  0.0641, -0.2985],\n",
            "         [ 0.1689, -0.6535,  0.4545,  ...,  0.1635, -0.0096, -0.5424],\n",
            "         ...,\n",
            "         [-0.1672,  0.0212,  0.5901,  ..., -0.0417, -0.0309, -0.3464],\n",
            "         [-0.1043, -0.0432,  0.6870,  ...,  0.0186, -0.0413, -0.5592],\n",
            "         [ 0.0080, -0.0725,  0.3609,  ..., -0.3591,  0.0622, -0.3122]]],\n",
            "       grad_fn=<NativeLayerNormBackward>)))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKTNonUJpHRg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83f02147-69d9-417b-acdb-0b60816624ea"
      },
      "source": [
        "print(embeddings[0])\n",
        "print(embeddings[2][-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[ 0.2329, -0.1494,  0.3605,  ..., -0.1701, -0.2105, -0.5222],\n",
            "         [-0.1091, -0.5142,  0.4510,  ...,  0.1104,  0.0641, -0.2985],\n",
            "         [ 0.1689, -0.6535,  0.4545,  ...,  0.1635, -0.0096, -0.5424],\n",
            "         ...,\n",
            "         [-0.1672,  0.0212,  0.5901,  ..., -0.0417, -0.0309, -0.3464],\n",
            "         [-0.1043, -0.0432,  0.6870,  ...,  0.0186, -0.0413, -0.5592],\n",
            "         [ 0.0080, -0.0725,  0.3609,  ..., -0.3591,  0.0622, -0.3122]]],\n",
            "       grad_fn=<NativeLayerNormBackward>)\n",
            "tensor([[[ 0.2329, -0.1494,  0.3605,  ..., -0.1701, -0.2105, -0.5222],\n",
            "         [-0.1091, -0.5142,  0.4510,  ...,  0.1104,  0.0641, -0.2985],\n",
            "         [ 0.1689, -0.6535,  0.4545,  ...,  0.1635, -0.0096, -0.5424],\n",
            "         ...,\n",
            "         [-0.1672,  0.0212,  0.5901,  ..., -0.0417, -0.0309, -0.3464],\n",
            "         [-0.1043, -0.0432,  0.6870,  ...,  0.0186, -0.0413, -0.5592],\n",
            "         [ 0.0080, -0.0725,  0.3609,  ..., -0.3591,  0.0622, -0.3122]]],\n",
            "       grad_fn=<NativeLayerNormBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1ey3WRuw4-7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae0a64bf-2b6a-4535-c14a-71d52e9f74f8"
      },
      "source": [
        "torch.stack(embeddings[2][-4:]).sum(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.2799, -0.2073,  0.4802,  ..., -0.5777, -0.3478, -0.3016],\n",
              "         [-0.3640, -2.3847,  0.4144,  ...,  2.0576,  0.6135,  0.9092],\n",
              "         [ 1.2091, -1.4786,  0.8424,  ...,  2.4500,  1.7974,  0.7206],\n",
              "         ...,\n",
              "         [-0.1366,  1.7886,  1.5123,  ...,  0.3546,  0.0368, -1.1302],\n",
              "         [ 0.6175,  1.4245,  2.4410,  ...,  0.9552, -0.0788, -2.9404],\n",
              "         [ 0.1089, -0.0056,  0.6044,  ..., -0.7702, -0.0874, -0.2162]]],\n",
              "       grad_fn=<SumBackward1>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    }
  ]
}